{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation using Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers have become the defacto standard for NLP tasks nowadays. They started being used in NLP but they are now being used in Computer Vision and sometimes to generate music as well. I am sure you would all have heard about the GPT3 Transformer or the jokes thereof.\n",
    "\n",
    "But everything aside, they are still hard to understand as ever. In my last post, I talked in quite a detail about transformers and how they work on a basic level. I went through the encoder and decoder architecture and the whole data flow in those different pieces of the neural network. \n",
    "\n",
    "But as I like to say we don't really understand something before we implement it ourselves. So in this post, we will implement an English to German language translator using Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Description\n",
    "\n",
    "We want to create a translator that uses transformers to convert English to German. So, if we look at it as a black-box, our network takes as input an English sentence and returns a German sentence.\n",
    "\n",
    "![](transformers/1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Optional, Any\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline\n",
    "# For data loading.\n",
    "from torchtext import data, datasets\n",
    "import spacy\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a translation Model,  we need translated sentence pairs between English and French. These are pretty much standard to get with the IWSLT(International Workshop on Spoken Language Translation) dataset which we can access using `torchtext.datasets`. This machine translation dataset is sort of the defacto standard used for translation tasks and has  translation of TED and TEDx talks on various topics in different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Spacy Models- These will be used for tokenization of german and english text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Spacy Models\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some special tokens we will use for specifying blank/padding words, and beginning and end of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special Tokens\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "BLANK_WORD = \"<blank>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining a preprocessing pipeline for both our source and target sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = data.Field(tokenize=tokenize_en, pad_token=BLANK_WORD)\n",
    "TGT = data.Field(tokenize=tokenize_de, init_token = BOS_WORD, \n",
    "                 eos_token = EOS_WORD, pad_token=BLANK_WORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the implemented function splits to divide our datasets into train,validation and test datasets. \n",
    "We also filter our sentences using the max_len parameter so that our code runs a lot faster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading wmt16_en_de.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wmt16_en_de.tar.gz: 503MB [00:33, 14.8MB/s] \n",
      "/Users/rajatbhatnagar/miniconda/envs/transformers/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 20\n",
    "train, val, test = datasets.translation.WMT14.splits(\n",
    "    exts=('.en', '.de'), fields=(SRC, TGT), \n",
    "    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN \n",
    "    and len(vars(x)['trg']) <= MAX_LEN)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let us try to see what our data looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example_0:(['Res@@', 'um@@', 'ption', 'of', 'the', 'session'], ['Wiederaufnahme', 'der', 'Sitzungsperiode'])\n",
      "Example_1:(['Please', 'rise', ',', 'then', ',', 'for', 'this', 'minute', '&', 'apos', ';', 's', 'silence', '.'], ['Ich', 'bitte', 'Sie', ',', 'sich', 'zu', 'einer', 'Schwei@@', 'ge@@', 'minute', 'zu', 'erheben', '.'])\n",
      "Example_2:(['(', 'The', 'House', 'rose', 'and', 'observed', 'a', 'minute', '&', 'apos', ';', 's', 'silence', ')'], ['(', 'Das', 'Parlament', 'erhebt', 'sich', 'zu', 'einer', 'Schwei@@', 'ge@@', 'minute', '.', ')'])\n",
      "Example_3:(['Madam', 'President', ',', 'on', 'a', 'point', 'of', 'order', '.'], ['Frau', 'Präsidentin', ',', 'zur', 'Geschäftsordnung', '.'])\n",
      "Example_4:(['If', 'the', 'House', 'agrees', ',', 'I', 'shall', 'do', 'as', 'Mr', 'Evans', 'has', 'suggested', '.'], ['Wenn', 'das', 'Haus', 'damit', 'einverstanden', 'ist', ',', 'werde', 'ich', 'dem', 'Vorschlag', 'von', 'Herrn', 'Evans', 'folgen', '.'])\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate([(x.src,x.trg) for x in train[0:5]]):\n",
    "    print(f\"Example_{i}:{example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a Source and Target Language vocaulary by using the built in function in data field object. We also specify a MIN_FREQ of 2 so that any word that doesn't occur atleast twice doesn't get to be a part of our vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FREQ = 2\n",
    "SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "TGT.build_vocab(train.trg, min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are done with this, we can simply use data.Bucketiterator which is used to giver batches of similar lengths to get our train iterator and validation iterator. Note that we use a batch_size of 1 for our validation data. Its optional to do this but is actually done so that we don't do padding or do minimal padding while checking validation data performance|.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajatbhatnagar/miniconda/envs/transformers/lib/python3.6/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE =350\n",
    "# Create iterators to process text in batches of approx. the same length\n",
    "train_iter = data.BucketIterator(train, batch_size=BATCH_SIZE, repeat=False, sort_key=lambda x: len(x.src))\n",
    "val_iter = data.BucketIterator(val, batch_size=1, repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what's in a batch. And what we are sending to the model as an input while training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajatbhatnagar/miniconda/envs/transformers/lib/python3.6/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 330,   63, 2237,  ...,    1,    1,    1],\n",
      "        [  38,    7,  347,  ...,    1,    1,    1],\n",
      "        [ 490,  774,  269,  ...,  307,    2,    1],\n",
      "        ...,\n",
      "        [  44, 2396,   43,  ...,    1,    1,    1],\n",
      "        [1749, 1255, 1873,  ..., 1367,   48,    2],\n",
      "        [2594,  137,   20,  ...,    1,    1,    1]]) torch.Size([350, 20])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "src_matrix = batch.src.T\n",
    "print(src_matrix, src_matrix.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2, 14532,   341,  ...,     1,     1,     1],\n",
      "        [    2,    44,   207,  ...,     1,     1,     1],\n",
      "        [    2,  2327, 21046,  ...,     4,     3,     1],\n",
      "        ...,\n",
      "        [    2,    52,   102,  ...,     1,     1,     1],\n",
      "        [    2,   450,  8266,  ...,    50,     4,     3],\n",
      "        [    2,    13, 17141,  ...,     3,     1,     1]]) torch.Size([350, 22])\n"
     ]
    }
   ],
   "source": [
    "trg_matrix = batch.trg.T\n",
    "print(trg_matrix, trg_matrix.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are these tensors? We can understand them better if we see their sizes first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so the src tensor contains 350 sentrences of length 20 and the target tensor is 350 sentences of length 22. Cool but what are the numbers they represent?We can understand them by looking at the source and target vocabulary. \n",
    "For example, the number 1 forms a big part of the src batch. What is 1 in src sentence? We can check it out using the vocab method index to string: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<blank>\n",
      "<s>\n",
      "<blank>\n"
     ]
    }
   ],
   "source": [
    "print(SRC.vocab.itos[1])\n",
    "print(TGT.vocab.itos[2])\n",
    "print(TGT.vocab.itos[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what about the 2 that occurs in all the trg sentence starting? You guessed correct it is the start token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TGT.vocab.stoi['</s>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: Please note that it really doesn't matter here if you do any other sort of preprocessing or use some other functions than `data.field` or use other tokenizers. What eventually matters is that in the end you need to send the sentence source and targets to your model in a way that's intended to be used by transformer. i.e. source sentences should be padded with blank token and target sentences need to have a start token, an end token and rest padded by blank tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have a way to send the input sentence and the shifted outputs to our transformer, we can look at creating the Transformer Ourself. A lot of the blocks here are taken from Pytorch nn module. Infact, Pytorech has a Transformer module too but it doesn't include a lot of functionalities present in the paper like the embedding layer, and the PositionalEncoding layer. So this is sort of a more complete implementation. \n",
    "\n",
    "We create our Transformer particularly using these various blocks from Pytorch nn module:\n",
    "\n",
    "- [TransformerEncoderLayer](https://pytorch.org/docs/master/generated/torch.nn.TransformerEncoderLayer.html) :  A single encoder layer\n",
    "- [TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html) : A stack of `num_encoder_layers` layers. In the paper it is by default kept as 6.\n",
    "- [TransformerDecoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html) : A single decoder layer\n",
    "- [TransformerDecoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html) : A stack of `num_decoder_layers` layers. In the paper it is by default kept as 6.\n",
    "\n",
    "If you want you can look at the source of all these blocks also. I had to look a many times into the sourcdes myself to make sure that I was giving the right inputs to these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "    \n",
    "class MyTransformer(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,\n",
    "                 num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: str = \"relu\",source_vocab_length: int = 60000,target_vocab_length: int = 60000) -> None:\n",
    "        super(MyTransformer, self).__init__()\n",
    "        self.source_embedding = nn.Embedding(source_vocab_length, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        encoder_norm = nn.LayerNorm(d_model)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "        self.target_embedding = nn.Embedding(target_vocab_length, d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "        self.out = nn.Linear(512, target_vocab_length)\n",
    "        self._reset_parameters()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        if src.size(1) != tgt.size(1):\n",
    "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
    "        src = self.source_embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        tgt = self.target_embedding(tgt)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask)\n",
    "        output = self.out(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can initialize the transformer and the optimizer using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-7cb3fbec7bd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_vocab_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_vocab_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_vocab_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_vocab_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.98\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda/envs/transformers/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \"\"\"\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/transformers/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/transformers/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/transformers/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \"\"\"\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/transformers/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "source_vocab_length = len(SRC.vocab)\n",
    "target_vocab_length = len(TGT.vocab)\n",
    "\n",
    "model = MyTransformer(source_vocab_length=source_vocab_length,target_vocab_length=target_vocab_length)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train using the simple train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, val_iter, model, optim, num_epochs,use_gpu=True): \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        # Train model\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            src = batch.src.cuda() if use_gpu else batch.src\n",
    "            trg = batch.trg.cuda() if use_gpu else batch.trg\n",
    "            #change to shape (bs , max_seq_len)\n",
    "            src = src.transpose(0,1)\n",
    "            #change to shape (bs , max_seq_len+1) , Since right shifted\n",
    "            trg = trg.transpose(0,1)\n",
    "            trg_input = trg[:, :-1]\n",
    "            targets = trg[:, 1:].contiguous().view(-1)\n",
    "            src_mask = (src != 0)\n",
    "            src_mask = src_mask.float().masked_fill(src_mask == 0, float('-inf')).masked_fill(src_mask == 1, float(0.0))\n",
    "            src_mask = src_mask.cuda() if use_gpu else src_mask\n",
    "            trg_mask = (trg_input != 0)\n",
    "            trg_mask = trg_mask.float().masked_fill(trg_mask == 0, float('-inf')).masked_fill(trg_mask == 1, float(0.0))\n",
    "            trg_mask = trg_mask.cuda() if use_gpu else trg_mask\n",
    "            size = trg_input.size(1)\n",
    "            #print(size)\n",
    "            np_mask = torch.triu(torch.ones(size, size)==1).transpose(0,1)\n",
    "            np_mask = np_mask.float().masked_fill(np_mask == 0, float('-inf')).masked_fill(np_mask == 1, float(0.0))\n",
    "            np_mask = np_mask.cuda() if use_gpu else np_mask   \n",
    "            # Forward, backprop, optimizer\n",
    "            optim.zero_grad()\n",
    "            preds = model(src.transpose(0,1), trg_input.transpose(0,1), tgt_mask = np_mask)#, src_mask = src_mask)#, tgt_key_padding_mask=trg_mask)\n",
    "            preds = preds.transpose(0,1).contiguous().view(-1, preds.size(-1))\n",
    "            loss = F.cross_entropy(preds,targets, ignore_index=0,reduction='sum')\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss += loss.item()/BATCH_SIZE\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(val_iter):\n",
    "                src = batch.src.cuda() if use_gpu else batch.src\n",
    "                trg = batch.trg.cuda() if use_gpu else batch.trg\n",
    "                #change to shape (bs , max_seq_len)\n",
    "                src = src.transpose(0,1)\n",
    "                #change to shape (bs , max_seq_len+1) , Since right shifted\n",
    "                trg = trg.transpose(0,1)\n",
    "                trg_input = trg[:, :-1]\n",
    "                targets = trg[:, 1:].contiguous().view(-1)\n",
    "                src_mask = (src != 0)\n",
    "                src_mask = src_mask.float().masked_fill(src_mask == 0, float('-inf')).masked_fill(src_mask == 1, float(0.0))\n",
    "                src_mask = src_mask.cuda() if use_gpu else src_mask\n",
    "                trg_mask = (trg_input != 0)\n",
    "                trg_mask = trg_mask.float().masked_fill(trg_mask == 0, float('-inf')).masked_fill(trg_mask == 1, float(0.0))\n",
    "                trg_mask = trg_mask.cuda() if use_gpu else trg_mask\n",
    "                size = trg_input.size(1)\n",
    "                #print(size)\n",
    "                np_mask = torch.triu(torch.ones(size, size)==1).transpose(0,1)\n",
    "                np_mask = np_mask.float().masked_fill(np_mask == 0, float('-inf')).masked_fill(np_mask == 1, float(0.0))\n",
    "                np_mask = np_mask.cuda() if use_gpu else np_mask\n",
    "\n",
    "                preds = model(src.transpose(0,1), trg_input.transpose(0,1), tgt_mask = np_mask)#, src_mask = src_mask)#, tgt_key_padding_mask=trg_mask)\n",
    "                preds = preds.transpose(0,1).contiguous().view(-1, preds.size(-1))         \n",
    "                loss = F.cross_entropy(preds,targets, ignore_index=0,reduction='sum')\n",
    "                valid_loss += loss.item()/1\n",
    "            \n",
    "        # Log after each epoch\n",
    "        print(f'''Epoch [{epoch+1}/{num_epochs}] complete. Train Loss: {train_loss/len(train_iter):.3f}. Val Loss: {valid_loss/len(val_iter):.3f}''')\n",
    "        \n",
    "        #Save best model till now:\n",
    "        if valid_loss/len(val_iter)<min(valid_losses,default=1e9): \n",
    "            print(\"saving state dict\")\n",
    "            torch.save(model.state_dict(), f\"checkpoint_best_epoch.pt\")\n",
    "        \n",
    "        train_losses.append(train_loss/len(train_iter))\n",
    "        valid_losses.append(valid_loss/len(val_iter))\n",
    "        \n",
    "        # Check Example after each epoch:\n",
    "        sentences = [\"This is an example to check how our model is performing.\"]\n",
    "        for sentence in sentences:\n",
    "            print(f\"Original Sentence: {sentence}\")\n",
    "            print(f\"Translated Sentence: {greeedy_decode_sentence(model,sentence)}\")\n",
    "    return train_losses,valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greeedy_decode_sentence(model,sentence):\n",
    "    model.eval()\n",
    "    sentence = SRC.preprocess(sentence)\n",
    "    indexed = []\n",
    "    for tok in sentence:\n",
    "        if SRC.vocab.stoi[tok] != 0 :\n",
    "            indexed.append(SRC.vocab.stoi[tok])\n",
    "        else:\n",
    "            indexed.append(0)\n",
    "    sentence = Variable(torch.LongTensor([indexed])).cuda()\n",
    "    trg_init_tok = TGT.vocab.stoi[BOS_WORD]\n",
    "    trg = torch.LongTensor([[trg_init_tok]]).cuda()\n",
    "    translated_sentence = \"\"\n",
    "    maxlen = 25\n",
    "    for i in range(maxlen):\n",
    "        size = trg.size(0)\n",
    "        np_mask = torch.triu(torch.ones(size, size)==1).transpose(0,1)\n",
    "        np_mask = np_mask.float().masked_fill(np_mask == 0, float('-inf')).masked_fill(np_mask == 1, float(0.0))\n",
    "        np_mask = np_mask.cuda()\n",
    "        pred = model(sentence.transpose(0,1), trg, tgt_mask = np_mask)\n",
    "        add_word = TGT.vocab.itos[pred.argmax(dim=2)[-1]]\n",
    "        translated_sentence+=\" \"+add_word\n",
    "        if add_word==EOS_WORD:\n",
    "            break\n",
    "        trg = torch.cat((trg,torch.LongTensor([[pred.argmax(dim=2)[-1]]]).cuda()))\n",
    "        #print(trg)\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35] complete. Train Loss: 86.092. Val Loss: 64.514\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Und die der der der der der der der der der der der der der der der der der der der der der der der\n",
      "Epoch [2/35] complete. Train Loss: 59.769. Val Loss: 55.631\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Das ist ein paar paar paar sehr , die das ist ein paar sehr Jahre . </s>\n",
      "Epoch [3/35] complete. Train Loss: 53.123. Val Loss: 49.685\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Das ist ein Beispiel , dass wir das Leben in der Welt ist , dass wir das Leben sein . </s>\n",
      "Epoch [4/35] complete. Train Loss: 47.825. Val Loss: 43.588\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , wie wir das Problem sehen , wie es eine Art ist . </s>\n",
      "Epoch [5/35] complete. Train Loss: 43.370. Val Loss: 39.692\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , wie wir das Problem sehen ist , wie unser Gehirn ein Problem ist . </s>\n",
      "Epoch [6/35] complete. Train Loss: 39.656. Val Loss: 37.369\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Das ist ein Beispiel , wie wir das Problem sehen , wie unser System in unserem Universum ist . </s>\n",
      "Epoch [7/35] complete. Train Loss: 36.550. Val Loss: 35.515\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier sehen wir ein Beispiel , wie unser Modell passiert . Das ist ein Modell . </s>\n",
      "Epoch [8/35] complete. Train Loss: 33.924. Val Loss: 33.309\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Das ist ein Beispiel , wie unser Modell ein Modell des Universums passiert . </s>\n",
      "Epoch [9/35] complete. Train Loss: 31.684. Val Loss: 33.285\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Das ist ein Beispiel , wie unser Modell das Modell des Modell betrachtet . Das ist ein Beispiel . </s>\n",
      "Epoch [10/35] complete. Train Loss: 29.740. Val Loss: 31.744\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier sehen wir ein Beispiel , wie unser Modell das Modell zeigt . Das passiert . </s>\n",
      "Epoch [11/35] complete. Train Loss: 28.056. Val Loss: 30.200\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , um zu schauen , wie unser Modell ein Modell ist . Das wir vorstellen . </s>\n",
      "Epoch [12/35] complete. Train Loss: 26.546. Val Loss: 29.962\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Das ist ein Beispiel , um zu schauen , wie unser Modell ein Modell ist . Das ist das möglich . </s>\n",
      "Epoch [13/35] complete. Train Loss: 25.198. Val Loss: 28.816\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , wie unser Modell aussieht . Das ist ein Modell . </s>\n",
      "Epoch [14/35] complete. Train Loss: 23.972. Val Loss: 28.994\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Das ist ein Beispiel , um zu schauen , wie unser Modell aussieht . Das ist ein Modell . </s>\n",
      "Epoch [15/35] complete. Train Loss: 22.850. Val Loss: 28.295\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier sehen wir ein Beispiel , wie unser Modell ein Modell zeigt . Das ist ein Modell . </s>\n",
      "Epoch [16/35] complete. Train Loss: 21.791. Val Loss: 28.000\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , um zu prüfen , wie unser Modell aussieht . Das ist ein Modell . </s>\n",
      "Epoch [17/35] complete. Train Loss: 20.834. Val Loss: 28.078\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier sehen wir ein Beispiel , wie unser Modell das zeigt . Das ist ein Modell , das wir erleben . </s>\n",
      "Epoch [18/35] complete. Train Loss: 19.902. Val Loss: 28.114\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , wie unser Modell zeigt . Das ist ein Modell . Sehen Sie . </s>\n",
      "Epoch [19/35] complete. Train Loss: 19.053. Val Loss: 27.876\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , wie unser Modell aussieht . Das ist ein Modell . </s>\n",
      "Epoch [20/35] complete. Train Loss: 18.237. Val Loss: 27.830\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , wie unser Modell zeigt , wie unser Modell kommuniziert . Das ist . </s>\n",
      "Epoch [21/35] complete. Train Loss: 17.445. Val Loss: 27.569\n",
      "saving state dict\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , um zu prüfen , wie unser Modell ein Modell ist . </s>\n",
      "Epoch [22/35] complete. Train Loss: 16.706. Val Loss: 28.366\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , um prüfen zu überprüfen , wie unser Modell ist . Ein Modell . </s>\n",
      "Epoch [23/35] complete. Train Loss: 15.984. Val Loss: 28.581\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , um zu prüfen , wie unser Modell ist . Wir spielen ein Beispiel . </s>\n",
      "Epoch [24/35] complete. Train Loss: 15.301. Val Loss: 28.979\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , um prüfen zu prüfen , wie unser Modell ist . Wir erleben das Modell . </s>\n",
      "Epoch [25/35] complete. Train Loss: 14.641. Val Loss: 29.850\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , um zu prüfen , wie unser Modell ist . Wir spielen . </s>\n",
      "Epoch [26/35] complete. Train Loss: 13.997. Val Loss: 28.727\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , um prüfen zu prüfen , wie unser Modell ist . Wir beobachten . </s>\n",
      "Epoch [27/35] complete. Train Loss: 13.365. Val Loss: 29.713\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , um überprüfen , wie unser Modell ist . Wie unser Modell kommuniziert . </s>\n",
      "Epoch [28/35] complete. Train Loss: 12.761. Val Loss: 29.441\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , warum unser Modell aussieht , wie unser Modell ist . Wir spielen . </s>\n",
      "Epoch [29/35] complete. Train Loss: 12.171. Val Loss: 29.452\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , um prüfen wie unser Modell ist . Sehen Sie . </s>\n",
      "Epoch [30/35] complete. Train Loss: 11.605. Val Loss: 30.445\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier sehen Sie ein Beispiel , wie unser Modell ein Modell spielt . </s>\n",
      "Epoch [31/35] complete. Train Loss: 11.062. Val Loss: 30.376\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , wie unser Modell aussieht , wie unser Modell kommuniziert . </s>\n",
      "Epoch [32/35] complete. Train Loss: 10.513. Val Loss: 30.622\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Dies ist ein Beispiel , um prüfen zu prüfen , wie unser Modell sich befindet . </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/35] complete. Train Loss: 9.994. Val Loss: 31.756\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , um prüfen wie unser Modell ist . Wir spielen . </s>\n",
      "Epoch [34/35] complete. Train Loss: 9.492. Val Loss: 31.005\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , um prüfen zu überprüfen , wie unser Modell ist . Wir spielen . </s>\n",
      "Epoch [35/35] complete. Train Loss: 9.014. Val Loss: 32.097\n",
      "Original Sentence: This is an example to check how our model is performing.\n",
      "Translated Sentence:  Hier ist ein Beispiel , um prüfen wie unser Modell ist . Wir spielen . </s>\n"
     ]
    }
   ],
   "source": [
    "train_losses,valid_losses = train(train_iter, val_iter, model, optim, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "losses = pd.DataFrame({'train_loss':train_losses,'val_loss':valid_losses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.092171</td>\n",
       "      <td>64.513807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59.769207</td>\n",
       "      <td>55.631315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53.123179</td>\n",
       "      <td>49.684637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.824730</td>\n",
       "      <td>43.588158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43.370320</td>\n",
       "      <td>39.692401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_loss   val_loss\n",
       "0   86.092171  64.513807\n",
       "1   59.769207  55.631315\n",
       "2   53.123179  49.684637\n",
       "3   47.824730  43.588158\n",
       "4   43.370320  39.692401"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=train_loss<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "train_loss",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "train_loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34
         ],
         "xaxis": "x",
         "y": [
          86.09217089393725,
          59.769206611495825,
          53.12317900754738,
          47.824729538826844,
          43.37032021594839,
          39.656007156276175,
          36.54975281349822,
          33.923692429042696,
          31.68389998846289,
          29.740134742714442,
          28.055855979328136,
          26.54632897800917,
          25.198211182561156,
          23.972044742681714,
          22.849652050003918,
          21.791462193694038,
          20.833555663244255,
          19.901720156339998,
          19.052981727194187,
          18.237195074802973,
          17.44500764230727,
          16.70578143819386,
          15.984481002238672,
          15.301127310897913,
          14.64095841611136,
          13.997070010776136,
          13.364856831519035,
          12.76055895827334,
          12.170979171644255,
          11.604901197199357,
          11.061791431185693,
          10.51321190874398,
          9.993775241501817,
          9.49157827095252,
          9.01432188637346
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=val_loss<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "val_loss",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "val_loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34
         ],
         "xaxis": "x",
         "y": [
          64.51380681656954,
          55.6313149326726,
          49.68463655312857,
          43.58815823396046,
          39.69240072651913,
          37.368812663513324,
          35.51517959310297,
          33.3089069617422,
          33.285287355958374,
          31.743639115283365,
          30.19961691111849,
          29.96208560759561,
          28.815588336660145,
          28.99433259001949,
          28.294898529220045,
          27.999661315533157,
          28.078422842109408,
          28.113776559997024,
          27.875994325102425,
          27.830495995178556,
          27.568998496155988,
          28.3655652700809,
          28.58051424591165,
          28.97867818171518,
          29.850407938371625,
          28.726781662096055,
          29.71316442008604,
          29.44119456148984,
          29.451842198037266,
          30.444568959035372,
          30.37648105182145,
          30.62240125986568,
          31.755532699195967,
          31.005417475156612,
          32.096650241132366
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"a8f752be-177b-4024-82fa-f6275590f617\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"a8f752be-177b-4024-82fa-f6275590f617\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'a8f752be-177b-4024-82fa-f6275590f617',\n",
       "                        [{\"hovertemplate\": \"variable=train_loss<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"train_loss\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"train_loss\", \"orientation\": \"v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34], \"xaxis\": \"x\", \"y\": [86.09217089393725, 59.769206611495825, 53.12317900754738, 47.824729538826844, 43.37032021594839, 39.656007156276175, 36.54975281349822, 33.923692429042696, 31.68389998846289, 29.740134742714442, 28.055855979328136, 26.54632897800917, 25.198211182561156, 23.972044742681714, 22.849652050003918, 21.791462193694038, 20.833555663244255, 19.901720156339998, 19.052981727194187, 18.237195074802973, 17.44500764230727, 16.70578143819386, 15.984481002238672, 15.301127310897913, 14.64095841611136, 13.997070010776136, 13.364856831519035, 12.76055895827334, 12.170979171644255, 11.604901197199357, 11.061791431185693, 10.51321190874398, 9.993775241501817, 9.49157827095252, 9.01432188637346], \"yaxis\": \"y\"}, {\"hovertemplate\": \"variable=val_loss<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"val_loss\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"val_loss\", \"orientation\": \"v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34], \"xaxis\": \"x\", \"y\": [64.51380681656954, 55.6313149326726, 49.68463655312857, 43.58815823396046, 39.69240072651913, 37.368812663513324, 35.51517959310297, 33.3089069617422, 33.285287355958374, 31.743639115283365, 30.19961691111849, 29.96208560759561, 28.815588336660145, 28.99433259001949, 28.294898529220045, 27.999661315533157, 28.078422842109408, 28.113776559997024, 27.875994325102425, 27.830495995178556, 27.568998496155988, 28.3655652700809, 28.58051424591165, 28.97867818171518, 29.850407938371625, 28.726781662096055, 29.71316442008604, 29.44119456148984, 29.451842198037266, 30.444568959035372, 30.37648105182145, 30.62240125986568, 31.755532699195967, 31.005417475156612, 32.096650241132366], \"yaxis\": \"y\"}],\n",
       "                        {\"legend\": {\"title\": {\"text\": \"variable\"}, \"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"index\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"value\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('a8f752be-177b-4024-82fa-f6275590f617');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.line(losses,y = ['train_loss','val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the best model using the saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just load model for inference\n",
    "model.load_state_dict(torch.load(f\"checkpoint_best_epoch.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the output on a single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ist es nicht einfach toll ? Bitte lassen Sie mich gerne in den Kommentare kennen . </s>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Isn't Natural language processing just awesome? Please do let me know in the comments.\"\n",
    "print(greeedy_decode_sentence(model,sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
